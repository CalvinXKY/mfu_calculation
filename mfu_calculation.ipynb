{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPi8awDlGHl13JQ5EE09NSZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CalvinXKY/mfu_calculation/blob/main/mfu_calculation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MFU快速计算器\n",
        "\n",
        "## 硬件配置"
      ],
      "metadata": {
        "id": "Fwy25fXe0_Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create a mfu calculation for LLM.\n",
        "\"\"\"\n",
        "# Hardware setting：\n",
        "GPU_FLOPS = 280 # 机器理论峰值 单位TFops/s\n",
        "GPU_NUMS = 1024"
      ],
      "metadata": {
        "id": "20fLIwertwv4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型参数配置"
      ],
      "metadata": {
        "id": "_mxeXAW_0x0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters eg：\n",
        "GBS = 1024\n",
        "SEQ_LEN = 4096\n",
        "HIDDEN_SIZE = 1024\n",
        "NUM_HEADS = 8\n",
        "D_MODEL = 128\n",
        "VOCAB_SIZE = 32768\n",
        "NUM_QUERY_GROUPS = 4\n",
        "FFN_HIDDEN_SIZE = 2048\n",
        "LAYER_NUMS = 100\n",
        "CP = 1\n",
        "STEP_TIME = 1.5\n",
        "\n",
        "# 非MoE 模型设置top_k=0, shared_experts=1\n",
        "SHARE_EXPERTS = 1\n",
        "TOP_K = 8"
      ],
      "metadata": {
        "id": "b32Q6Vg06uqG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 计算函数定义："
      ],
      "metadata": {
        "id": "FY2vkF1m4XN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 简化版：\n",
        "def mfu_calculation(step_time=STEP_TIME,\n",
        "             gbs=GBS,\n",
        "             seq_len=SEQ_LEN,\n",
        "             hidden_size=HIDDEN_SIZE,\n",
        "             vocab_size=VOCAB_SIZE,\n",
        "             num_heads=NUM_HEADS,\n",
        "             d_model=D_MODEL,\n",
        "             num_query_groups=NUM_QUERY_GROUPS,\n",
        "             ffn_hidden_size=FFN_HIDDEN_SIZE,\n",
        "             share=SHARE_EXPERTS,\n",
        "             top_k=TOP_K,\n",
        "             layer_nums=LAYER_NUMS,\n",
        "             context_parallel=CP,\n",
        "             mlp_with_gate=True):\n",
        "    embedding_flops = gbs * seq_len * hidden_size * vocab_size\n",
        "\n",
        "    # attention flops\n",
        "    q_linear = gbs * seq_len * hidden_size ** 2\n",
        "    kv_linear = 2 * gbs * seq_len * hidden_size * num_query_groups * d_model\n",
        "    kv_scores = gbs * seq_len**2 * num_heads * d_model * (context_parallel + 1) / (2 * context_parallel)\n",
        "    v_projection = gbs * seq_len**2 * num_heads * d_model * (context_parallel + 1) / (2 * context_parallel)\n",
        "\n",
        "    out_linear = gbs * seq_len * hidden_size ** 2\n",
        "    attention_layer_flops = q_linear + kv_linear + kv_scores + v_projection + out_linear\n",
        "\n",
        "    # consider layer norm. (It can be ignored)\n",
        "    layer_norm = gbs * seq_len * hidden_size\n",
        "\n",
        "    if mlp_with_gate:\n",
        "        # llama structure\n",
        "        mlp_layer_flops = 3 * gbs * seq_len * hidden_size * ffn_hidden_size\n",
        "    else:\n",
        "        mlp_layer_flops = 2 * gbs * seq_len * hidden_size * ffn_hidden_size\n",
        "\n",
        "    moe_layer_flops = mlp_layer_flops * (share + top_k)\n",
        "    model_flops = 3 * 2 * (embedding_flops + layer_nums * (attention_layer_flops + moe_layer_flops + layer_norm))\n",
        "\n",
        "    mfu = model_flops / (GPU_NUMS * step_time * (10 ** 12)) / GPU_FLOPS\n",
        "    return mfu"
      ],
      "metadata": {
        "id": "3S4AiF0eA2a7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 计算测试"
      ],
      "metadata": {
        "id": "-3TSl-Qp4cQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mfu_calculation(step_time=1.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtVTqvV0TkS6",
        "outputId": "78027f58-cdf6-4eb5-f563-09809aaca844"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4007877972553143"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deepseek v2/v3模型的计算：降MHA替换为MLA\n",
        "\n",
        "注意忽略如下计算：\n",
        "* 旋转编码/RMSNorm\n",
        "* 低精度运算\n",
        "* 重计算参数"
      ],
      "metadata": {
        "id": "8visJML7FiR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 简化版：\n",
        "def mfu_calculation_deepseek(step_time=STEP_TIME,\n",
        "               gbs=GBS,\n",
        "               seq_len=SEQ_LEN,\n",
        "               hidden_size=HIDDEN_SIZE,\n",
        "               vocab_size=VOCAB_SIZE,\n",
        "               num_heads=NUM_HEADS,\n",
        "               d_model=D_MODEL,\n",
        "               num_query_groups=NUM_QUERY_GROUPS,\n",
        "               ffn_hidden_size=FFN_HIDDEN_SIZE,\n",
        "               share=SHARE_EXPERTS,\n",
        "               top_k=TOP_K,\n",
        "               layer_nums=LAYER_NUMS,\n",
        "               context_parallel=CP,\n",
        "               q_lora_rank=None,\n",
        "               kv_lora_rank=None,\n",
        "               mlp_with_gate=True):\n",
        "    embedding_flops = gbs * seq_len * hidden_size * vocab_size\n",
        "\n",
        "    # attention flops\n",
        "    if q_lora_rank is not None:\n",
        "      q_down_proj = gbs * seq_len * hidden_size * q_lora_rank\n",
        "      q_up_proj  = gbs * seq_len * q_lora_rank * num_heads * d_model\n",
        "      q_linear = q_down_proj + q_up_proj\n",
        "    else:\n",
        "      q_linear = gbs * seq_len * hidden_size ** 2\n",
        "\n",
        "    if kv_lora_rank is not None:\n",
        "      kv_down_proj = gbs * seq_len * hidden_size * kv_lora_rank\n",
        "      kv_up_proj = gbs * seq_len * kv_lora_rank * num_heads * d_model * 2\n",
        "      kv_linear = kv_down_proj + kv_up_proj\n",
        "    else:\n",
        "      kv_linear = gbs * seq_len * hidden_size * num_query_groups * d_model * 2\n",
        "\n",
        "    kv_scores = gbs * seq_len**2 * num_heads * d_model * (context_parallel + 1) / (2 * context_parallel)\n",
        "    v_projection = gbs * seq_len**2 * num_heads * d_model * (context_parallel + 1) / (2 * context_parallel)\n",
        "\n",
        "    out_linear = gbs * seq_len * hidden_size ** 2\n",
        "    attention_layer_flops = q_linear + kv_linear + kv_scores + v_projection + out_linear\n",
        "\n",
        "    # consider layer norm. (It can be ignored)\n",
        "    layer_norm = gbs * seq_len * hidden_size\n",
        "\n",
        "    if mlp_with_gate:\n",
        "        # llama structure\n",
        "        mlp_layer_flops = 3 * gbs * seq_len * hidden_size * ffn_hidden_size\n",
        "    else:\n",
        "        mlp_layer_flops = 2 * gbs * seq_len * hidden_size * ffn_hidden_size\n",
        "\n",
        "    moe_layer_flops = mlp_layer_flops * (share + top_k)\n",
        "    model_flops = 3 * 2 * (embedding_flops + layer_nums * (attention_layer_flops + moe_layer_flops + layer_norm))\n",
        "\n",
        "    mfu = model_flops / (GPU_NUMS * step_time * (10 ** 12)) / GPU_FLOPS\n",
        "    return mfu"
      ],
      "metadata": {
        "id": "GOddzrGko9yE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mfu_calculation_deepseek(step_time=1.5, q_lora_rank=256, kv_lora_rank=128)"
      ],
      "metadata": {
        "id": "d6osexCYq2rd",
        "outputId": "74d92a2b-fc5f-407a-aff7-2da39cc1b7b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3938851712438857"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}