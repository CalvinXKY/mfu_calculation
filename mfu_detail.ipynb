{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP9ZHHK4KEIkimuuII5FecT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CalvinXKY/mfu_calculation/blob/main/mfu_detail.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "本内容主要介绍transformer架构中一些操作层的flops计算量。\n",
        "\n",
        "![transformer decoder架构]\n",
        "\n",
        "# 基本模块flops计算\n",
        "## 线性层的计算量\n",
        "\n",
        "线性层的计算公式为 Y = wX + b 涉及到矩阵的乘法与加法运算。\n",
        "\n",
        "矩阵乘法与加法的flops的计算为：\n",
        "\n",
        "**乘法计算量**：对于两个矩阵A和B的乘法C=AB，其中A是m×n矩阵，B是n×p矩阵，C是m×p矩阵。每个元素Cij需要进行 n 次乘法和n-1次加法，总共有mp个元素，因此总FLOPS为：\n",
        "\n",
        "mp(n+(n-1)) = 2mnp - mp。\n",
        "\n",
        "**加法/减法计算量**：对于两个矩阵A和B的加法C=A+B，其中A和B都是m×n矩阵，C也是m×n矩阵。每个元素Cij需要进行一次加法，总共有mn个元素，因此总FLOPS为mn。\n",
        "\n",
        "对于linear计算，里面涉及一个矩阵乘和一个矩阵加法，由于元素需要展平再运算，权重w的维度[m, n] 输入的维度是[1, n] 输出维度[1, m]，其计算量为\n",
        "\n",
        "2mn\n",
        "\n",
        "不考虑bias的计算量为\n",
        "\n",
        "2mn  - m\n",
        "\n",
        "对于transformer的线性层输入与输出一般用相同的大小，形状都为：[batch_size, seq_len, d_model],\n",
        "线性层的创建一般使用 Linear(hidden_size, hidden_size, bias=False)\n",
        "所以计算量为：\n",
        "\n",
        "flops = 2 * batch_size * seq_len * hidden_size * hidden_size\n",
        "\n",
        "如果不一致时：\n",
        "flops = 2 * batch_size * seq_len * size_1 * size_2"
      ],
      "metadata": {
        "id": "odaFtoySwx8B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpi5qP3QYdtw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KWjh_Y9fwgfY"
      },
      "outputs": [],
      "source": [
        "def calcu_linear_flops(batch_size, seq_len, hidden_size, head=0, d_model=0,bias=False):\n",
        "    bias_flops = 0 if not bias else batch_size * seq_len * hidden_size\n",
        "    if head ==0:\n",
        "        flops = 2 * batch_size * seq_len * hidden_size * hidden_size + bias_flops\n",
        "    else:\n",
        "        flops = 2 * batch_size * seq_len * hidden_size * head * d_model + bias_flops\n",
        "    return flops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention模块的计算量\n",
        "\n",
        "一般的MHA(MultiHeadAttention)计算的构造如下：\n",
        "```\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(input_dim, output_dim)\n",
        "        self.key = nn.Linear(input_dim, output_dim)\n",
        "        self.value = nn.Linear(input_dim, output_dim)\n",
        "        self.dk = output_dim\n",
        "\n",
        "    # Scaled dot-product attention:\n",
        "    def self_attention(self, query, key, value, mask):\n",
        "        # query/key/value:  (bs,  seq_len, dk)/(bs, heads, seq_len, dk)\n",
        "        # mask shape = (bs, 1, seq_len)/(bs, 1, 1, seq_len)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.dk)) # (bs, seq_len, seq_len)/(bs, heads, seq_len, seq_len)\n",
        "        if mask is not None:\n",
        "            scores.masked_fill_(mask == torch.tensor(False), float(\"-inf\"))\n",
        "        # Softmax dim=-1 stands for apply the softmax along the last dimension\n",
        "        attention_weights = nn.Softmax(dim=-1)(scores)  # (bs, heads, seq_len, seq_len)/(bs, seq_len, seq_len)\n",
        "        attention_qkv = torch.matmul(attention_weights, value)   # (bs, seq_len, dk)/(bs, heads, seq_len, dk)\n",
        "        return attention_qkv\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        # qkv shape: (bs, seq_len, d_model)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)\n",
        "        value = self.value(value)\n",
        "        attention_qkv = self.self_attention(query, key, value, mask)  # shape:  (bs, seq_len, d_model)\n",
        "        return attention_qkv\n",
        "\n",
        "class MultiHeadedAttention(Attention):\n",
        "    def __init__(self, d_model, heads):\n",
        "        super().__init__(d_model, d_model)\n",
        "        assert d_model % heads == 0\n",
        "        self.dk = d_model // heads  # head dimension\n",
        "        self.heads = heads\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "        self.sqrt_dk = torch.sqrt(torch.tensor(self.dk))\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        batch_size = query.shape[0]\n",
        "        # qkv shape: (bs, seq_len, dk*heads)\n",
        "        # dk * heads = d_model\n",
        "        query = self.query(query).view(batch_size, -1, self.heads, self.dk).transpose(1, 2)\n",
        "        key = self.key(key).view(batch_size, -1, self.heads, self.dk).transpose(1, 2)\n",
        "        value = self.value(value).view(batch_size, -1, self.heads, self.dk).transpose(1, 2)\n",
        "        attention_qkv = self.self_attention(query, key, value, mask)  # shape:  (bs, heads, seq_len, dk)\n",
        "        #  (bs, heads, seq_len, dk) -> (bs, seq_len, dk*heads)\n",
        "        reshaped = attention_qkv.transpose(1, 2).reshape(batch_size, -1, self.heads * self.dk)\n",
        "        representations_batch = self.out_linear(reshaped)\n",
        "        return representations_batch\n",
        "```\n",
        "\n",
        "主要运算：\n",
        "* Q/K/V 线性映射\n",
        "* scores 中QK乘法运算\n",
        "* attention_qkv 中V和attention_weights乘法运算\n",
        "* out_linear 线性度计算\n",
        "\n",
        "次要运算：\n",
        "* softmax 计算\n",
        "* masked_fill 计算\n",
        "\n",
        "对于主要运算中有个需要考虑点：\n",
        "* Attention的变化：query_attention的计算KV的heads数量与Q的heads数量不一致。\n",
        "* 序列并行（context parallel/ring attention）: 考虑并行度。\n",
        "\n",
        "\n",
        "次要运算在估算flops时通常可以忽略，这里例出其计算方式：\n",
        "\n",
        "softmax的flops计算量： 输入的shape：(bs, heads, seq_len, seq_len)\n",
        "元素计算涉及指数运算、加法运算、除法运算。 计算量：\n",
        "\n",
        "   3 * bs * heads * seq_len * (seq_len - 1)\n",
        "\n",
        "maked_fill是一个掩模操作包含：判断操作和赋值操作，假设是需要遍历整个矩阵，每个元素操作一次，而赋值操作仅对需要操作的元素赋值，输入矩阵的大小为[bs, heads, seq_len, seq_len], 操作的个数为X。所以计算量：\n",
        "\n",
        "\n",
        "   bs * heads *  seq_len * seq_len + X\n",
        "\n",
        "由于X操作相对来说较小, 公式简化为：\n",
        "\n",
        "   bs * heads *  seq_len * seq_len"
      ],
      "metadata": {
        "id": "zzJMXD-s7MFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_attention_flops(batch_size, seq_len, heads, d_model, num_query_groups=0, context_parallel=1):\n",
        "    num_query_groups = num_query_groups if num_query_groups != 0 else heads\n",
        "    q_linear_flops = calcu_linear_flops(batch_size, seq_len, heads * d_model)\n",
        "    k_linear_flops = calcu_linear_flops(batch_size, seq_len, heads * d_model, num_query_groups, d_model)\n",
        "    v_linear_flops = k_linear_flops\n",
        "\n",
        "    kv_scores_flops = 2 * batch_size * seq_len**2 * heads * d_model * (context_parallel + 1) / (2 * context_parallel)\n",
        "    mask_flops = batch_size * heads *  seq_len * seq_len\n",
        "    softmax_flops = 3 * batch_size * heads * seq_len * (seq_len - 1)\n",
        "\n",
        "    qkv_flops = kv_scores_flops\n",
        "    out_linear_flops = calcu_linear_flops(batch_size, seq_len, heads * d_model)\n",
        "    return q_linear_flops + k_linear_flops + v_linear_flops + kv_scores_flops + mask_flops + softmax_flops + qkv_flops + out_linear_flops"
      ],
      "metadata": {
        "id": "5azr-HUe7K_u"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding和LayerNorm\n",
        "\n",
        "embedding层主要完成映射计算, 输入维度[batch_size, seq_len] 输出维度： (bs, seq_len, d_model)。flops计算可以忽略。\n",
        "\n",
        "layer normal的计算内容一般如下：\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "\n",
        "def layer_normalization(x, epsilon=1e-8):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True) # 最后一个维度\n",
        "    std = np.std(x, axis=-1, keepdims=True)\n",
        "    normalized_x = (x - mean) / (std + epsilon)\n",
        "    return normalized_x\n",
        "\n",
        "```\n",
        "假设数据的长度为L\n",
        "包含平均值计算、标准差计算、偏移计算；\n",
        "* mean计算包含L加法和一次除法：  L + 1\n",
        "* std计算，每个元素进行一个减法、一个乘法、一个加法。最后进行一个除法和一个乘法操作： 3*L + 2\n",
        "* 标准化：每个元素一次减法、一次除法操作： 2*L\n",
        "\n",
        "\n",
        "所以操作计算量：\n",
        "\n",
        "6 * batch_size * seq_len * hidden_size + 3\n",
        "\n",
        "vocab_size是字典大小，一般而言数值较大，所以计算时需要考虑。layer normal计算量相对较小"
      ],
      "metadata": {
        "id": "wTw0eoK49BQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_layer_norm_flops(batch_size, seq_len, hidden_size):\n",
        "  return 6 * batch_size * seq_len * hidden_size  + 3"
      ],
      "metadata": {
        "id": "m-FXTKEsUk-G"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP/FFN层的计算量\n",
        "\n",
        "MLP层的构建常见的方式如下：\n",
        "\n",
        "```\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, dff=2048):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, dff)\n",
        "        self.linear2 = nn.Linear(dff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, representations_batch):\n",
        "        return self.linear2(self.relu(self.linear1(representations_batch)))\n",
        "```\n",
        "\n",
        "主要包含两个线性层操作和一个relu计算。\n",
        "\n",
        "输入/输出: [batch_size, seq_len, hidden_size]\n",
        "dff值：ffn_hidden_size\n",
        "\n",
        "计算量为两次线性运算 + 一个relu操作，其flops操作数量如下：\n",
        "2 * batch_size * seq_len * hidden_size * ffn_hidden_size + batch_size * seq_len * ffn_hidden_size\n",
        "\n",
        "如果是采用SwiGLU(如llama3结构)，一般的计算包含三次线性运算，一个silu运算，一个元素乘法运算。\n",
        "\n",
        "3 * batch_size * seq_len * hidden_size * ffn_hidden_size + 2 * batch_size * seq_len * ffn_hidden_size\n",
        "\n"
      ],
      "metadata": {
        "id": "qGcCHN9FWAqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_mlp_flops(batch_size, seq_len, hidden_size, ffn_hidden_size, use_gate=True):\n",
        "  \"\"\"\n",
        "   use_gate=True SwiGLU structure FFN.\n",
        "  \"\"\"\n",
        "  if use_gate:\n",
        "    flops = 3 * 2 * batch_size * seq_len * hidden_size * ffn_hidden_size + 2 * batch_size * seq_len * ffn_hidden_size\n",
        "  else:\n",
        "    flops = 2 * 2 * batch_size * seq_len * hidden_size * ffn_hidden_size + batch_size * seq_len * ffn_hidden_size\n",
        "  return flops"
      ],
      "metadata": {
        "id": "xuKwm6uRWZI5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logits计算\n",
        "\n",
        "logits计算包含三个运算：\n",
        "\n",
        "* layernorm\n",
        "* linaer，(词表映射)\n",
        "* softmax\n",
        "\n",
        "对应尺寸\n",
        "* layernorm: [batch_size, seq_len, hidden_size]\n",
        "* linear: input:[batch_size，seq_len*hidden_size] output: :[batch_size，seq_len*vocab_size]\n",
        "* softmax: [batch_size，seq_len*vocab_size]\n",
        "\n",
        "对应计算量：\n",
        "\n",
        "6 * batch_size * seq_len * hidden_size  + 3\n",
        "\n",
        "batch_size * seq_len * hidden_size * vocab_size\n",
        "\n",
        "3 * batch_size * seq_len * (vocab_size - 1)"
      ],
      "metadata": {
        "id": "t6DlpZmofGTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_logits_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size):\n",
        "    layer_norm_flops = 6 * batch_size * seq_len * hidden_size  + 3\n",
        "\n",
        "    linear_flops = 2 * batch_size * seq_len * hidden_size * vocab_size\n",
        "\n",
        "    softmax_flos = 3 * batch_size * seq_len * (vocab_size - 1)\n",
        "    return layer_norm_flops + linear_flops + softmax_flos"
      ],
      "metadata": {
        "id": "k5JJMN9RMi3e"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 典型训练模型的flops计算\n",
        "\n",
        "训练估算约定:\n",
        "\n",
        "模型backward计算是forward计算大约两倍, 因为需要计算输入 + 权重的梯度[参考](https://arxiv.org/pdf/2205.05198)。\n",
        "\n",
        "## GPT结构\n",
        "\n",
        "模型涉及计算的主要结构\n",
        "\n",
        "decoder_layer x L + logtis\n",
        "\n",
        "其中L是层数，decoder构成：\n",
        "\n",
        "MHA + FFN + 2 LayerNorm\n",
        "\n"
      ],
      "metadata": {
        "id": "8fQ7-5fyNC-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def caclu_gpt_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size, ffn_hidden_size, layer_nums):\n",
        "  attention_flops = calcu_attention_flops(batch_size, seq_len, heads, d_model, num_query_groups=0, context_parallel=1)\n",
        "  ffn_flops = calcu_mlp_flops(batch_size, seq_len, hidden_size, ffn_hidden_size, use_gate=False)\n",
        "  layer_norm_flops = calcu_layer_norm_flops(batch_size, seq_len, hidden_size)\n",
        "  logits_flops = calcu_logits_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size)\n",
        "  return 3 * (logits_flops + (layer_norm_flops * 2 + attention_flops + ffn_flops) * layer_nums)"
      ],
      "metadata": {
        "id": "w71MVb2hHifU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLAMA结构\n",
        "\n",
        "结构：\n",
        "\n",
        "(GMQA + FFN + RMSNorm) x L + logtis\n",
        "\n",
        "其中GMQA 是group attention， FFN： Feed ForwardSwiGLU结构，RMSNorm的计算量用layernorm估算"
      ],
      "metadata": {
        "id": "BkKN5SASMSqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def caclu_llama_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size, ffn_hidden_size, layer_nums, num_query_groups):\n",
        "  attention_flops = calcu_attention_flops(batch_size, seq_len, heads, d_model, num_query_groups=num_query_groups, context_parallel=1)\n",
        "  ffn_flops = calcu_mlp_flops(batch_size, seq_len, hidden_size, ffn_hidden_size, use_gate=True)\n",
        "  layer_norm_flops = calcu_layer_norm_flops(batch_size, seq_len, hidden_size)\n",
        "  logits_flops = calcu_logits_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size)\n",
        "  return 3 * (logits_flops + (layer_norm_flops * 2 + attention_flops + ffn_flops) * layer_nums)"
      ],
      "metadata": {
        "id": "ssOaoymHN6Ug"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MoE模型\n",
        "\n",
        "在llama结构基础上ffn增加topk专家数量系数，计算公式：\n",
        "\n",
        "(GMQA + FFN * Experts_topk + RMSNorm) x L + logtis\n",
        "\n"
      ],
      "metadata": {
        "id": "8jvl60XLLo9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def caclu_moe_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size, ffn_hidden_size, layer_nums, num_query_groups, topk):\n",
        "  attention_flops = calcu_attention_flops(batch_size, seq_len, heads, d_model, num_query_groups=num_query_groups, context_parallel=1)\n",
        "  ffn_flops = calcu_mlp_flops(batch_size, seq_len, hidden_size, ffn_hidden_size, use_gate=True)\n",
        "  layer_norm_flops = calcu_layer_norm_flops(batch_size, seq_len, hidden_size)\n",
        "  logits_flops = calcu_logits_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size)\n",
        "  return 3 * (logits_flops + (layer_norm_flops * 2 + attention_flops + ffn_flops * topk) * layer_nums)"
      ],
      "metadata": {
        "id": "Xis5osj3PURF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MFU计算\n",
        "\n",
        "MFU(Model Flops Utilization)计算的公式为：\n",
        "\n",
        "mfu = 单位时间实际flops/单位时间名义flops\n",
        "\n",
        "单位时间实际flops = 单步模型计算flops总数/单步迭代时间\n",
        "\n",
        "mfu = model_flops_sum / iter_time * (device_peak_flops * device_num)\n",
        "\n",
        "通常device_peak_flops的单位为： TFlops/s"
      ],
      "metadata": {
        "id": "HdfKN3xcPtR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_moe_mfu(iter_time, batch_size, seq_len, heads, d_model, hidden_size, vocab_size, ffn_hidden_size, layer_nums, num_query_groups, topk, device_nums, device_peak_flops):\n",
        "    model_flops = caclu_moe_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size, ffn_hidden_size, layer_nums, num_query_groups, topk)\n",
        "    return model_flops / (iter_time * device_peak_flops * device_nums * 10 ** 12)\n"
      ],
      "metadata": {
        "id": "9MSWETZ2RUmS"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试："
      ],
      "metadata": {
        "id": "iCFd5ZZyStTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calcu_moe_mfu(iter_time=1.5,\n",
        "       batch_size=1024, seq_len=4096, heads=8, d_model=128,\n",
        "       hidden_size=1024, vocab_size=32768, ffn_hidden_size=2048,\n",
        "       layer_nums=100, num_query_groups=4, topk=9, device_nums=1024, device_peak_flops=280)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hGazgz3Sx8A",
        "outputId": "a639359b-08d5-4346-86c0-14a84976dca1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40131207486903275"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 参考文献：\n",
        "\n",
        "https://zhuanlan.zhihu.com/p/691126108\n",
        "\n",
        "https://github.com/naklecha/llama3-from-scratch/blob/main/README.md\n",
        "\n",
        "https://arxiv.org/pdf/2205.05198\n",
        "\n",
        "llama结构图片：\n",
        "https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/te_llama/tutorial_accelerate_hf_llama_with_te.html"
      ],
      "metadata": {
        "id": "DFnkp-9VWZcF"
      }
    }
  ]
}